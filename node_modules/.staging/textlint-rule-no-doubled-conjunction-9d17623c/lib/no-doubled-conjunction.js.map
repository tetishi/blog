{"version":3,"sources":["../src/no-doubled-conjunction.js"],"names":["context","options","helper","RuleHelper","Syntax","report","getSource","RuleError","Paragraph","node","isChildNode","Link","Image","BlockQuote","Emphasis","source","StringSource","text","toString","isSentenceNode","type","SentenceSyntax","Sentence","sentences","charRegExp","filter","length","then","tokenizer","selectConjenction","sentence","tokens","tokenizeForSentence","raw","conjunctionTokens","token","pos","prev_token","map","reduce","prev","current","current_tokens","prev_sentence","prev_tokens","surface_form","originalIndex","originalIndexFromPosition","line","loc","start","column","word_position","padding","index"],"mappings":"AAAA;AACA;;;;;;;AACA;;AACA;;AACA;;AACA;;;;AAEA;;;;;;;;AAQe,kBAAUA,OAAV,EAAiC;AAAA,MAAdC,OAAc,uEAAJ,EAAI;AAC5C,MAAMC,MAAM,GAAG,IAAIC,8BAAJ,CAAeH,OAAf,CAAf;AACA,MAAM;AAACI,IAAAA,MAAD;AAASC,IAAAA,MAAT;AAAiBC,IAAAA,SAAjB;AAA4BC,IAAAA;AAA5B,MAAyCP,OAA/C;AACA,SAAO;AACH,KAACI,MAAM,CAACI,SAAR,EAAmBC,IAAnB,EAAwB;AACpB,UAAIP,MAAM,CAACQ,WAAP,CAAmBD,IAAnB,EAAyB,CAACL,MAAM,CAACO,IAAR,EAAcP,MAAM,CAACQ,KAArB,EAA4BR,MAAM,CAACS,UAAnC,EAA+CT,MAAM,CAACU,QAAtD,CAAzB,CAAJ,EAA+F;AAC3F;AACH;;AACD,UAAMC,MAAM,GAAG,IAAIC,6BAAJ,CAAiBP,IAAjB,CAAf;AACA,UAAMQ,IAAI,GAAGF,MAAM,CAACG,QAAP,EAAb;;AACA,UAAMC,cAAc,GAAIV,IAAD,IAAUA,IAAI,CAACW,IAAL,KAAcC,yBAAeC,QAA9D;;AACA,UAAIC,SAAS,GAAG,6BAAeN,IAAf,EAAqB;AACjCO,QAAAA,UAAU,EAAE;AADqB,OAArB,EAEbC,MAFa,CAENN,cAFM,CAAhB,CAPoB,CAUpB;AACA;;AACA,UAAGI,SAAS,CAACG,MAAV,KAAqB,CAAxB,EAA2B;AACvB;AACH;;AACD,aAAO,+BAAeC,IAAf,CAAoBC,SAAS,IAAI;AACtC,YAAMC,iBAAiB,GAAIC,QAAD,IAAc;AACtC,cAAIC,MAAM,GAAGH,SAAS,CAACI,mBAAV,CAA8BF,QAAQ,CAACG,GAAvC,CAAb;AACA,cAAIC,iBAAiB,GAAGH,MAAM,CAACN,MAAP,CAAeU,KAAD,IAAWA,KAAK,CAACC,GAAN,KAAc,KAAvC,CAAxB;AACA,iBAAO,CAACN,QAAD,EAAWI,iBAAX,CAAP;AACD,SAJD;;AAKA,YAAIG,UAAU,GAAG,IAAjB;AACAd,QAAAA,SAAS,CAACe,GAAV,CAAcT,iBAAd,EAAiCU,MAAjC,CAAwC,CAACC,IAAD,EAAOC,OAAP,KAAmB;AACzD,cAAIN,KAAK,GAAGE,UAAZ;AACA,cAAI,CAACP,QAAD,EAAWY,cAAX,IAA6BD,OAAjC;AACA,cAAI,CAACE,aAAD,EAAgBC,WAAhB,IAA+BJ,IAAnC;;AACA,cAAII,WAAW,IAAIA,WAAW,CAAClB,MAAZ,GAAqB,CAAxC,EAA2C;AACzCS,YAAAA,KAAK,GAAGS,WAAW,CAAC,CAAD,CAAnB;AACD;;AACD,cAAIF,cAAc,CAAChB,MAAf,GAAwB,CAA5B,EAA+B;AAC7B,gBAAIS,KAAK,IAAIO,cAAc,CAAC,CAAD,CAAd,CAAkBG,YAAlB,KAAmCV,KAAK,CAACU,YAAtD,EAAoE;AAClE,kBAAIC,aAAa,GAAG/B,MAAM,CAACgC,yBAAP,CAAiC;AACnDC,gBAAAA,IAAI,EAAElB,QAAQ,CAACmB,GAAT,CAAaC,KAAb,CAAmBF,IAD0B;AAEnDG,gBAAAA,MAAM,EAAErB,QAAQ,CAACmB,GAAT,CAAaC,KAAb,CAAmBC,MAAnB,IAA6BT,cAAc,CAAC,CAAD,CAAd,CAAkBU,aAAlB,GAAkC,CAA/D;AAF2C,eAAjC,CAApB,CADkE,CAKlE;;AACA,kBAAIC,OAAO,GAAG;AACVC,gBAAAA,KAAK,EAAER;AADG,eAAd;AAGAzC,cAAAA,MAAM,CAACI,IAAD,EAAO,IAAIF,SAAJ,iHAAoC8C,OAApC,CAAP,CAAN;AACD;AACF;;AACDhB,UAAAA,UAAU,GAAGF,KAAb;AACA,iBAAOM,OAAP;AACD,SAtBD;AAuBD,OA9BM,CAAP;AA+BH;;AA/CE,GAAP;AAiDH;;AAAA","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\";\nimport {getTokenizer} from \"kuromojin\";\nimport {split as splitSentences, Syntax as SentenceSyntax} from \"sentence-splitter\";\nimport StringSource from \"textlint-util-to-string\";\n\n/*\n    1. Paragraph Node -> text\n    2. text -> sentences\n    3. tokenize sentence\n    4. report error if found word that match the rule.\n\n    TODO: need abstraction\n */\nexport default function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    const {Syntax, report, getSource, RuleError} = context;\n    return {\n        [Syntax.Paragraph](node){\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const source = new StringSource(node);\n            const text = source.toString();\n            const isSentenceNode = (node) => node.type === SentenceSyntax.Sentence;\n            let sentences = splitSentences(text, {\n                charRegExp: /[。\\?\\!？！]/\n            }).filter(isSentenceNode);\n            // if not have a sentence, early return\n            // It is for avoiding error of emptyArray.reduce().\n            if(sentences.length === 0) {\n                return;\n            }\n            return getTokenizer().then(tokenizer => {\n              const selectConjenction = (sentence) => {\n                let tokens = tokenizer.tokenizeForSentence(sentence.raw);\n                let conjunctionTokens = tokens.filter((token) => token.pos === \"接続詞\");\n                return [sentence, conjunctionTokens];\n              };\n              let prev_token = null;\n              sentences.map(selectConjenction).reduce((prev, current) => {\n                let token = prev_token;\n                let [sentence, current_tokens] = current;\n                let [prev_sentence, prev_tokens] = prev;\n                if (prev_tokens && prev_tokens.length > 0) {\n                  token = prev_tokens[0];\n                }\n                if (current_tokens.length > 0) {\n                  if (token && current_tokens[0].surface_form === token.surface_form) {\n                    let originalIndex = source.originalIndexFromPosition({\n                      line: sentence.loc.start.line,\n                      column: sentence.loc.start.column + (current_tokens[0].word_position - 1)\n                    });\n                    // padding position\n                    var padding = {\n                        index: originalIndex\n                    };\n                    report(node, new RuleError(`同じ接続詞が連続して使われています。`, padding));\n                  }\n                }\n                prev_token = token;\n                return current;\n              });\n            });\n        }\n    }\n};\n"],"file":"no-doubled-conjunction.js"}