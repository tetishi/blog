{"version":3,"sources":["../src/max-ten.js"],"names":["defaultOptions","max","strict","isSandwichedMeishi","before","token","after","undefined","pos","addPositions","base","relative","line","column","module","exports","context","options","maxLen","isStrict","helper","RuleHelper","Syntax","RuleError","report","getSource","Paragraph","node","isChildNode","BlockQuote","sentences","charRegExp","newLineCharacters","then","tokenizer","forEach","sentence","text","value","source","Source","currentTenCount","tokens","tokenizeForSentence","lastToken","index","surface","surface_form","isSandwiched","positionInSentence","indexToPosition","word_position","positionInNode","loc","start","ruleError"],"mappings":"AAAA;AACA;;AACA;;AACA;;AACA;;AACA;;;;AACA,IAAMA,cAAc,GAAG;AACnBC,EAAAA,GAAG,EAAE,CADc;AACX;AACRC,EAAAA,MAAM,EAAE,KAFW,CAEL;;AAFK,CAAvB;;AAKA,SAASC,kBAAT,OAIG;AAAA,MAJyB;AACxBC,IAAAA,MADwB;AAExBC,IAAAA,KAFwB;AAGxBC,IAAAA;AAHwB,GAIzB;;AACC,MAAIF,MAAM,KAAKG,SAAX,IAAwBD,KAAK,KAAKC,SAAlC,IAA+CF,KAAK,KAAKE,SAA7D,EAAwE;AACpE,WAAO,KAAP;AACH;;AACD,SAAOH,MAAM,CAACI,GAAP,KAAe,IAAf,IAAuBF,KAAK,CAACE,GAAN,KAAc,IAA5C;AACH;AACD;;;;;;;;;AAOA,SAASC,YAAT,CAAsBC,IAAtB,EAA4BC,QAA5B,EAAsC;AAClC,SAAO;AACHC,IAAAA,IAAI,EAAEF,IAAI,CAACE,IAAL,GAAYD,QAAQ,CAACC,IAArB,GAA4B,CAD/B;AACkC;AACrCC,IAAAA,MAAM,EAAEF,QAAQ,CAACC,IAAT,IAAiB,CAAjB,GAAqBF,IAAI,CAACG,MAAL,GAAcF,QAAQ,CAACE,MAA5C,CAAmD;AAAnD,MACFF,QAAQ,CAACE,MAHZ,CAGiC;;AAHjC,GAAP;AAKH;AACD;;;;;;AAIAC,MAAM,CAACC,OAAP,GAAiB,UAASC,OAAT,EAAgC;AAAA,MAAdC,OAAc,uEAAJ,EAAI;AAC7C,MAAMC,MAAM,GAAGD,OAAO,CAAChB,GAAR,IAAeD,cAAc,CAACC,GAA7C;AACA,MAAMkB,QAAQ,GAAGF,OAAO,CAACf,MAAR,IAAkBF,cAAc,CAACE,MAAlD;AACA,MAAIkB,MAAM,GAAG,IAAIC,8BAAJ,CAAeL,OAAf,CAAb;AACA,MAAI;AAACM,IAAAA,MAAD;AAASC,IAAAA,SAAT;AAAoBC,IAAAA,MAApB;AAA4BC,IAAAA;AAA5B,MAAyCT,OAA7C;AACA,SAAO;AACH,KAACM,MAAM,CAACI,SAAR,EAAmBC,IAAnB,EAAwB;AACpB,UAAIP,MAAM,CAACQ,WAAP,CAAmBD,IAAnB,EAAyB,CAACL,MAAM,CAACO,UAAR,CAAzB,CAAJ,EAAmD;AAC/C;AACH;;AACD,UAAIC,SAAS,GAAG,6BAAeL,SAAS,CAACE,IAAD,CAAxB,EAAgC;AAC5CI,QAAAA,UAAU,EAAE,WADgC;AAE5CC,QAAAA,iBAAiB,EAAE;AAFyB,OAAhC,CAAhB;AAIA;;;;;;;AAMA;;;;;;;AAMA,aAAO,+BAAeC,IAAf,CAAoBC,SAAS,IAAI;AACpCJ,QAAAA,SAAS,CAACK,OAAV,CAAkBC,QAAQ,IAAI;AAC1B,cAAIC,IAAI,GAAGD,QAAQ,CAACE,KAApB;AACA,cAAIC,MAAM,GAAG,IAAIC,yBAAJ,CAAWH,IAAX,CAAb;AACA,cAAII,eAAe,GAAG,CAAtB;AACA,cAAIC,MAAM,GAAGR,SAAS,CAACS,mBAAV,CAA8BN,IAA9B,CAAb;AACA,cAAIO,SAAS,GAAG,IAAhB;AACAF,UAAAA,MAAM,CAACP,OAAP,CAAe,CAAC9B,KAAD,EAAQwC,KAAR,KAAkB;AAC7B,gBAAIC,OAAO,GAAGzC,KAAK,CAAC0C,YAApB;;AACA,gBAAID,OAAO,KAAK,GAAhB,EAAqB;AACjB;AACA,kBAAIE,YAAY,GAAG7C,kBAAkB,CAAC;AAClCC,gBAAAA,MAAM,EAAEsC,MAAM,CAACG,KAAK,GAAG,CAAT,CADoB;AAElCxC,gBAAAA,KAAK,EAAEA,KAF2B;AAGlCC,gBAAAA,KAAK,EAAEoC,MAAM,CAACG,KAAK,GAAG,CAAT;AAHqB,eAAD,CAArC,CAFiB,CAOjB;;AACA,kBAAI,CAAC1B,QAAD,IAAa6B,YAAjB,EAA+B;AAC3B;AACH;;AACDP,cAAAA,eAAe;AACfG,cAAAA,SAAS,GAAGvC,KAAZ;AACH;;AACD,gBAAIyC,OAAO,KAAK,GAAhB,EAAqB;AACjB;AACAL,cAAAA,eAAe,GAAG,CAAlB;AACH,aAnB4B,CAoB7B;;;AACA,gBAAIA,eAAe,IAAIvB,MAAvB,EAA+B;AAC3B,kBAAI+B,kBAAkB,GAAGV,MAAM,CAACW,eAAP,CAAuBN,SAAS,CAACO,aAAV,GAA0B,CAAjD,CAAzB;AACA,kBAAIC,cAAc,GAAG3C,YAAY,CAAC2B,QAAQ,CAACiB,GAAT,CAAaC,KAAd,EAAqBL,kBAArB,CAAjC;AACA,kBAAIM,SAAS,GAAG,IAAIvC,OAAO,CAACO,SAAZ,yDAAkCL,MAAlC,mEAAsD;AAClEN,gBAAAA,IAAI,EAAEwC,cAAc,CAACxC,IAAf,GAAsB,CADsC;AAElEC,gBAAAA,MAAM,EAAEuC,cAAc,CAACvC;AAF2C,eAAtD,CAAhB;AAIAW,cAAAA,MAAM,CAACG,IAAD,EAAO4B,SAAP,CAAN;AACAd,cAAAA,eAAe,GAAG,CAAlB;AACH;AACJ,WA/BD;AAgCH,SAtCD;AAuCH,OAxCM,CAAP;AAyCH;;AA9DE,GAAP;AAgEH,CArED","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\"\nimport {getTokenizer} from \"kuromojin\";\nimport {split as splitSentences} from \"sentence-splitter\";\nimport Source from \"structured-source\";\nconst defaultOptions = {\n    max: 3, // 1文に利用できる最大の、の数\n    strict: false // 例外ルールを適応するかどうか\n};\n\nfunction isSandwichedMeishi({\n    before,\n    token,\n    after\n}) {\n    if (before === undefined || after === undefined || token === undefined) {\n        return false;\n    }\n    return before.pos === \"名詞\" && after.pos === \"名詞\";\n}\n/**\n * add two positions.\n * note: line starts with 1, column starts with 0.\n * @param {Position} base\n * @param {Position} relative\n * @return {Position}\n */\nfunction addPositions(base, relative) {\n    return {\n        line: base.line + relative.line - 1, // line 1 + line 1 should be line 1\n        column: relative.line == 1 ? base.column + relative.column // when the same line\n            : relative.column               // when another line\n    };\n}\n/**\n * @param {RuleContext} context\n * @param {object} [options]\n */\nmodule.exports = function(context, options = {}) {\n    const maxLen = options.max || defaultOptions.max;\n    const isStrict = options.strict || defaultOptions.strict;\n    let helper = new RuleHelper(context);\n    let {Syntax, RuleError, report, getSource} = context;\n    return {\n        [Syntax.Paragraph](node){\n            if (helper.isChildNode(node, [Syntax.BlockQuote])) {\n                return;\n            }\n            let sentences = splitSentences(getSource(node), {\n                charRegExp: /[。\\?\\!？！]/,\n                newLineCharacters: \"\\n\\n\"\n            });\n            /*\n             <p>\n             <str><code><img><str>\n             <str>\n             </p>\n             */\n            /*\n             # workflow\n             1. split text to sentences\n             2. sentence to tokens\n             3. check tokens\n             */\n            return getTokenizer().then(tokenizer => {\n                sentences.forEach(sentence => {\n                    let text = sentence.value;\n                    let source = new Source(text);\n                    let currentTenCount = 0;\n                    let tokens = tokenizer.tokenizeForSentence(text);\n                    let lastToken = null;\n                    tokens.forEach((token, index) => {\n                        let surface = token.surface_form;\n                        if (surface === \"、\") {\n                            // 名詞に囲まわれている場合は例外とする\n                            let isSandwiched = isSandwichedMeishi({\n                                before: tokens[index - 1],\n                                token: token,\n                                after: tokens[index + 1]\n                            });\n                            // strictなら例外を例外としない\n                            if (!isStrict && isSandwiched) {\n                                return;\n                            }\n                            currentTenCount++;\n                            lastToken = token;\n                        }\n                        if (surface === \"。\") {\n                            // reset\n                            currentTenCount = 0;\n                        }\n                        // report\n                        if (currentTenCount >= maxLen) {\n                            let positionInSentence = source.indexToPosition(lastToken.word_position - 1);\n                            let positionInNode = addPositions(sentence.loc.start, positionInSentence);\n                            let ruleError = new context.RuleError(`一つの文で\"、\"を${maxLen}つ以上使用しています`, {\n                                line: positionInNode.line - 1,\n                                column: positionInNode.column\n                            });\n                            report(node, ruleError);\n                            currentTenCount = 0;\n                        }\n                    });\n                });\n            });\n        }\n    }\n}\n"],"file":"max-ten.js"}